import ldap
import base64
import re
import os

import hashlib
import requests
import signal
import sys

import gzip

from urllib.parse import urlparse
from colorama import Fore, Style

# Possible Colorama Colors (Foreground)
# -------------------------------------
# BLACK
# BLUE
# CYAN
# GREEN
# LIGHTBLACK_EX
# LIGHTBLUE_EX
# LIGHTCYAN_EX
# LIGHTGREEN_EX
# LIGHTMAGENTA_EX
# LIGHTRED_EX
# LIGHTWHITE_EX
# LIGHTYELLOW_EX
# MAGENTA
# RED
# RESET
# WHITE
# YELLOW

# Below Data Would Transform To http://10.0.0.16:8180/ExecTemplateJDK5.class
# objectClass: javaNamingReference
# javaCodeBase: http://10.0.0.16:8180/
# javaFactory: ExecTemplateJDK5

# Customization Options
uri_base_colors: str = f"{Fore.LIGHTGREEN_EX}%s{Fore.LIGHTRED_EX}%s"
header_color: str = f"{Fore.LIGHTBLACK_EX}"
info_color: str = f"{Fore.LIGHTYELLOW_EX}"
error_color: str = f"{Fore.RED}%s: "
warning_color: str = f"{Fore.RED}%s: "
generic_error_value_color: str = f"{Fore.LIGHTYELLOW_EX}"
generic_warning_value_color: str = f"{Fore.LIGHTYELLOW_EX}"
key_value_color: str = f"{Fore.GREEN}%s{Fore.LIGHTBLACK_EX}: {Fore.RED}%s"

# Functional Options
log_dir: str = "logs"  # Directory Where Logs Are Stored
output_dir: str = "output"
downloaded_jndi_log: str = "jndi.log"
ldap_timeout: int = 10  # Seconds
requests_timeout: int = 10  # Seconds

# File Download Timeout
headers: dict = {
        "User-Agent": "log4j"  # I don't currently know Log4J's User Agent If Any
}


def strip_non_ascii_chars(string: str) -> str:
    return string.encode("ascii", "ignore").decode()

def print_results(uri: str, base: str, results):
    for entry_base, entry in results:
        print(f"{header_color}%s" % ("-"*40))
        print(f"{uri_base_colors}" % (strip_non_ascii_chars(uri), strip_non_ascii_chars(base)))
        print(f"{header_color}%s" % ("-"*40))
        for key in entry:
            print(f"{key_value_color}" % (strip_non_ascii_chars(key).strip(), entry[key][0].decode("ascii", "ignore").strip()))
        print(f"{header_color}%s{Style.RESET_ALL}" % ("-"*40))

def grab_urls(results) -> list:
    url_list: list = []
    for entry_base, entry in results:
        javaCodeBase = None
        javaFactory = None
        
        # If Not Known URL Data Type, Skip
        if "objectClass" in entry and entry["objectClass"][0].decode("ascii", "ignore").strip() != "javaNamingReference":
            print(f"{warning_color} {generic_warning_value_color}%s" % ("Unknown Object Class", entry["objectClass"][0].decode("ascii", "ignore").strip()))
            continue
        
        # Domain For Malicious Code
        if "javaCodeBase" in entry:
            javaCodeBase = entry["javaCodeBase"][0].decode("ascii", "ignore").strip()
        
        # Path For Malicious Code
        if "javaFactory" in entry:
            javaFactory = entry["javaFactory"][0].decode("ascii", "ignore").strip()
        
        if javaCodeBase is not None and javaFactory is not None:
            url_list.append("%s%s.class" % (javaCodeBase, javaFactory))
            
    return url_list

def download_file(url: str, dir_path: str, file_name: str):
    if (os.path.exists(os.path.join(dir_path, file_name))):
        print(f"{warning_color}%s{Style.RESET_ALL}" % ("URL Already Downloaded", url))
        return
    
    print(f"{info_color}Downloading %s..." % url)
    try:
        response = requests.get(url, headers=headers, verify=False, timeout=requests_timeout)
    except:
        print(f"{error_color}{generic_error_value_color}%s{Style.RESET_ALL}" % ("Failed To Download Payload", url))
        return
    
    if (response.status_code != 200):
        print(f"{warning_color}{uri_base_colors} - Code: %s - Proceeding anyway...{Style.RESET_ALL}" % ("Status Code", uri, base, response.status_code))
    
    print(f"{info_color}Saving %s to %s" % (url, os.path.join(dir_path, file_name)))
    save_file(dir_path=dir_path, file_name=file_name, file_bytes=response.content)

def save_file(dir_path: str, file_name: str, file_bytes):
    # Make Directories
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)
    
    with open(os.path.join(dir_path, file_name), mode="wb") as f:
        f.write(file_bytes)

def download_urls(ldap_uri, urls):
    uri_base_encoded: bytes = base64.urlsafe_b64encode(bytes(ldap_uri, "ascii", "ignore"))
    uri_base_encoded: str = os.path.join(output_dir, hashlib.sha512(uri_base_encoded).hexdigest())
    
    print(f"{header_color}%s" % ("-"*40))
    print(f"{info_color}Downloading Payloads From: %s" % (ldap_uri))
    print(f"{header_color}%s" % ("-"*40))
    for url in urls:
        # TODO: Make Dir From uri and base after base64 encoding
        url_encoded: bytes = base64.urlsafe_b64encode(bytes(url, "ascii", "ignore"))
        url_encoded: str = "%s.malware" % hashlib.sha512(url_encoded).hexdigest()
        
        download_file(url=url, dir_path=uri_base_encoded, file_name=url_encoded)
    print(f"{header_color}%s" % ("-"*40))

def remove_substring(substring: str, original_string: str):
    compiled = re.compile(re.escape(substring), re.IGNORECASE)
    return compiled.sub("", original_string)

def strip_obfuscation(string: str):
    # Custom Function To Make Case-Insensitive
    string = remove_substring("${lower:", string)
    string = remove_substring("${upper:", string)
    string = remove_substring("${", string)
    string = remove_substring("}", string)
    string = remove_substring("::-", string)
    
    #print(string)
    return string

def read_logs():
    # TODO: Consider Turning Into Python Generator
    jndi_regex: str = '((?:jndi)?(?::)?(?:%3A)?(?:ldap)[^ "]*)'  # See https://stackoverflow.com/a/3513858/6828099 about non-capturing groups
    jndi_regex_compiled = re.compile(jndi_regex, re.IGNORECASE)

    log_files = next(os.walk(log_dir), (None, None, []))[2]
    #print(log_files)
    
    malicious_ldap_urls: list = []
    for log_file in log_files:
        if log_file.endswith(".gz"):
            # If Log File Is GZip, Then Use GZip To Read File
            log = gzip.open(os.path.join(log_dir, log_file), "r")
        else:
            # If Log File Is Straight Up On Disk, Then Just Read Like Normal
            log = open(os.path.join(log_dir, log_file), mode="r")

        lines = log.read().splitlines()
        for line in lines:
            try:
                # Some lines can be treated as an array of bytes
                line = line.decode()
            except (UnicodeDecodeError, AttributeError):
                pass
            
            line = strip_obfuscation(string=line)
            matches = re.findall(jndi_regex_compiled, line)
            if len(matches) > 0:
                #print("Line: %s" % line)
                #print("Matches: %s" % matches)
                malicious_ldap_urls.extend(matches)
    
    cleaned_urls: list = []
    for url in malicious_ldap_urls:
        # Filter Out Non-LDAP URLs
        if "ldap://" not in url:
            continue
        
        url = url.lstrip("jndi:")
        url = url.lstrip(":")

        #print("Malicious URL: %s" % url)
        cleaned_urls.append(url)
    
    return cleaned_urls

def attempt_download_payloads(uri: str, base: str):
    # Equivalent Command For URL: ldap://127.0.0.1:1389/togbnj
    # ldapsearch -h 127.0.0.1:1389 -x -b togbnj
    
    try:
        # Connect To Server
        conn = ldap.initialize(uri)
        conn.set_option(ldap.OPT_NETWORK_TIMEOUT, ldap_timeout)
        conn.set_option(ldap.OPT_TIMEOUT, ldap_timeout)
        
        conn.simple_bind_s()
    except ldap.SERVER_DOWN:
        print(f"{error_color}{uri_base_colors}{Style.RESET_ALL}" % ("Server Down: Couldn't Connect To", uri, base))
        return
    except:
        print(f"{error_color}{uri_base_colors}{Style.RESET_ALL}" % ("Exception: Couldn't Connect To", uri, base))
        return

    # Search For Malicious Payload
    try:
        results = conn.search_s(base=base, scope=ldap.SCOPE_SUBTREE)
    except ldap.NO_SUCH_OBJECT:
        print(f"{error_color}{generic_error_value_color}%s{Style.RESET_ALL}" % ("No Such Object", base))
        return
    except:
        print(f"{error_color}{generic_error_value_color}%s{Style.RESET_ALL}" % ("Exception Looking Up Object", base))
        return

    # Print Results
    print_results(uri=uri, base=base, results=results)
    
    # Grab URLs
    urls = grab_urls(results=results)
    
    # Download Payloads
    download_urls(ldap_uri=("%s%s" % (uri, base)), urls=urls)

def handler(signum, frame):
    # I may end up capturing the signal as an exception to do proper cleanup while also enforcing near immediate exit.
    print(f"{info_color}Exiting...")
    
    # exit(0) or sys.exit(0)
    os._exit(0)  # This one exits immediately

if __name__ == "__main__":
    # To Capture Signal Handler To Make Exiting Easier
    signal.signal(signal.SIGINT, handler)
    
    # Check For Already Scanned Servers To Save Time And Not Download Again
    url_log = open(downloaded_jndi_log, mode="a+")
    url_log.seek(0)  # Move To Beginning Of File To Read In Already Downloaded URLs
    downloaded_urls = url_log.read().splitlines()

    # The "while True" simulates tail in that it'll constantly monitor the file.
    # The existing downloaded_urls list will keep from downloading the same files repeatedly
    while True:
        malicious_ldap_urls = read_logs()
        for url in malicious_ldap_urls:
            # To Speed Up Testing
            #if "1228316559" not in url:
                #continue

            # If URL Scanned, Don't Scan Again
            if url.strip() in downloaded_urls:
                continue

            parsed_url = urlparse(url)
            uri: str = ("%s://%s" % (parsed_url.scheme, parsed_url.netloc)).rstrip("/") + "/"
            base: str = "%s" % parsed_url.path.lstrip("/")  # There's also params, query, and fragment
            print(f"{info_color}Attempting To Download Payloads From: {uri_base_colors}{Style.RESET_ALL}" % (uri, base))
            
            #print("%s - %s" % (uri, base))
            attempt_download_payloads(uri=uri, base=base)
            
            downloaded_urls.append(url)  # Add Downloaded URL To List To Avoid Downloading Again (May Remove/Modify Later For Long Term Running)
            url_log.write("%s\n" % url.strip())

            # This is to handle the tee command screwing with logging (will implement proper fix later)
            url_log.flush()  # If Needing To Save To Disk Immediately, Uncomment Line (Will Slow Down IO)
